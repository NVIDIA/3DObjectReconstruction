{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Object Reconstruction using NVBundleSDF \n",
    "\n",
    "## Overview \n",
    "\n",
    "This guide demonstrates an end-to-end real2sim workflow for reconstructing 3D objects from stereo video input using state-of-the-art computer vision and neural rendering techniques. The pipeline combines:\n",
    "\n",
    "- **[FoundationStereo](https://arxiv.org/abs/2501.09898)** for depth estimation from stereo image pairs\n",
    "- **[SAM2](https://arxiv.org/abs/2408.00714)** serves as the object segmentation for the entire video\n",
    "- **[BundleSDF](https://arxiv.org/abs/2303.14158)** for the real-world scale textured mesh generation\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"../data/docs/pipeline_overview.png\" alt=\"Pipeline Overview\" title=\"3D Object Reconstruction Workflow\">\n",
    "</div>\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll learn how to perform 3D object reconstruction by:\n",
    "\n",
    "- **Data Preparation**: Importing stereo input data and pre-processing it for optimal reconstruction\n",
    "- **Depth Estimation**: Using FoundationStereo to generate accurate depth maps from stereo pairs\n",
    "- **Object Segmentation**: Employing SAM2 to segment and track objects across all frames\n",
    "- **3D Object Reconstruction**: Leveraging BundleSDF for pose estimation, SDF training, mesh extraction, and texture baking\n",
    "- **Asset Creation**: Creating textured 3D assets ready for downstream applications such as digital content creation, dataset simulation and object pose estimation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### System Requirements\n",
    "- **GPU**: NVIDIA GPU with CUDA support (minimum requirements: Compute Capability 7.0 with at least 24GB VRAM)\n",
    "- **Memory**: 32GB+ RAM recommended\n",
    "- **Storage**: 100GB+ free space recommended\n",
    "- **OS**: Ubuntu 22.04+\n",
    "- **Software**: \n",
    "  - [Docker](https://docs.docker.com/engine/install/) with [nvidia-container-runtime](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) enabled\n",
    "  - [Docker Compose](https://docs.docker.com/compose/install/)\n",
    "\n",
    "### Initial Setup\n",
    "In the cell below, we will install additional dependencies for video codecs, GStreamer plugins, and build the SAM2 extension that we'll use later in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies for video processing and build SAM2 extension\n",
    "print(\"Installing additional dependencies...\")\n",
    "import subprocess, os, pathlib, sys\n",
    "DEEPSTREAM_SCRIPT = pathlib.Path(\"/opt/nvidia/deepstream/deepstream/user_additional_install.sh\")\n",
    "if DEEPSTREAM_SCRIPT.exists():\n",
    "    subprocess.check_call([\"bash\", str(DEEPSTREAM_SCRIPT)])\n",
    "SAM2_ROOT = pathlib.Path(os.getenv(\"SAM2_ROOT\", \"/sam2\"))\n",
    "if SAM2_ROOT.exists():\n",
    "    subprocess.check_call([sys.executable, \"setup.py\", \"build_ext\", \"--inplace\"], cwd=SAM2_ROOT)\n",
    "else:\n",
    "    print(\"⚠️  SAM2 root not found – skipping C++ extension build.\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation and Guidelines\n",
    "\n",
    "The notebook comes with a sample dataset and a base configuration file that allows users to optimize their results. This section covers suitable object types and dataset capturing instructions.\n",
    "\n",
    "### Recommended Object Types\n",
    "\n",
    "For optimal reconstruction results, choose objects with the following characteristics:\n",
    "\n",
    "- **Rigid, Non-Deformable Objects**: The workflow performs best with objects that maintain a fixed shape across frames\n",
    "- **Rich Surface Texture**: High texture variance enables reliable feature detection and matching, which is critical for accurate reconstruction\n",
    "- **Asymmetrical Objects**: Distinct content on different faces helps avoid ambiguity during feature matching\n",
    "- **Opaque Materials**: Avoid transparent or translucent materials (glass, clear plastic) as they interfere with depth and feature consistency\n",
    "\n",
    "### Dataset Capturing Guidelines\n",
    "\n",
    "To achieve optimal 3D reconstruction quality, follow these steps when capturing object images. The goal is to ensure complete coverage of the object's geometry while maintaining consistency in framing and orientation.\n",
    "\n",
    "#### 1. Position the Object\n",
    "- **Centering**: Place the object in the center of the camera frame\n",
    "- **Size**: The object should occupy roughly 45-65% of the image area - large enough to capture details while providing context\n",
    "- **Lighting**: \n",
    "  - Use even, diffused lighting to minimize harsh shadows and reflections\n",
    "  - Avoid backlighting or direct overhead lights that create glare or overexposure\n",
    "  - Ensure the object is well-lit from multiple directions to reveal surface details\n",
    "\n",
    "#### 2. Capture the First Set (Primary Faces)\n",
    "- Begin image capture while slowly rotating the object horizontally in one direction (clockwise or counterclockwise)\n",
    "- Cover approximately 360 degrees of rotation\n",
    "- This step should expose four visible faces of the object: front, back, and both sides\n",
    "- Capture multiple overlapping frames to ensure robust feature matching across angles\n",
    "\n",
    "#### 3. Capture the Second Set (Remaining Faces)\n",
    "- Flip the object to reveal previously hidden faces (typically top and bottom)\n",
    "- Continue capturing images while rotating slightly, following the same pattern as the previous step\n",
    "- Ensure full coverage of the remaining two faces with overlapping views for consistent alignment\n",
    "\n",
    "The animation below demonstrates the full object rotation and flip process, showing how to cover all six faces in a consistent, controlled manner.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"../data/docs/adv.gif\" alt=\"Capture Example\" title=\"Capture Example\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "### Manual Capture Guidelines (Alternative Method)\n",
    "\n",
    "If a turntable is not available, you can capture the data manually by following these guidelines:\n",
    "\n",
    "#### 1. Scan-like Movement\n",
    "- Hold the object in your hands and rotate it manually in front of the camera\n",
    "- Treat the process like scanning the object: gradually expose all surfaces to the camera\n",
    "- Rotate the object slowly and smoothly, ensuring sufficient visual overlap between consecutive frames\n",
    "\n",
    "#### 2. Maximize Visible Surface Area\n",
    "- Ensure the camera can see a large portion of the object's surface in each frame\n",
    "- Avoid fast or jerky movements - slower rotations help the system track features accurately\n",
    "- Verify that all six faces of the object (front, back, sides, top, and bottom) are captured clearly\n",
    "\n",
    "#### 3. Maintain Consistent Distance\n",
    "- Keep the object at a consistent distance from the camera throughout the capture process\n",
    "- Avoid moving the object significantly closer or farther during capture\n",
    "\n",
    "The animation below demonstrates effective manual object capture technique.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"../data/docs/input_dino.gif\" alt=\"Manual Capture Example\" title=\"Manual Capture Example\" width=\"600\">\n",
    "</div>\n",
    "  \n",
    "## Comprehensive Camera Comparison\n",
    "\n",
    "* Note that the following information is for reference only. Please check your camera's parameters for accurate 3D object reconstruction.\n",
    "\n",
    "| **Specification** | **ZED 2i Camera (Stereolabs)** | **QooCam EGO 3D Camera (Kandao)** | **Hawk Stereo Camera (Leopard Imaging)** | **ZED Mini Camera (Stereolabs)** |\n",
    "|-------------------|--------------------------------|-----------------------------------|------------------------------------------|----------------------------------|\n",
    "| **Manufacturer** | Stereolabs | Kandao | Leopard Imaging | Stereolabs |\n",
    "| **Resolution** | 2K stereo | 4K image / 2K video | Industrial grade | HD stereo |\n",
    "| **Form Factor** | Desktop setup | Lightweight, portable | Compact industrial | Ultra-compact, lightweight |\n",
    "| **Field of View** | Wide | Standard | Standard | Standard |\n",
    "| **Target Audience** | Developers, robotics | Consumer/prosumer | Industrial, NVIDIA ecosystem | Mixed-reality, robotics |\n",
    "| **Best Use Case** | Desktop capture, robotics | Quick field captures, handheld | Industrial applications, Isaac ROS | Mixed-reality, compact robotics |\n",
    "| **Technical Specifications** | | | | |\n",
    "| **Focal Length (fx)** | 1070.800 | 3079.6 | 958.35 | 522.38 |\n",
    "| **Focal Length (fy)** | 1070.700 | 3075.1 | 956.18 | 522.38 |\n",
    "| **Principal Point (cx)** | 1098.950 | 2000.0 | 959.36 | 644.88 |\n",
    "| **Principal Point (cy)** | 649.044 | 1500.0 | 590.95 | 356.03 |\n",
    "| **Baseline (m)** | 0.1198 | 0.0658 | 0.1495 | 0.12 |\n",
    "| **Eye Separation** | Standard | Standard | Standard | 6.5cm |\n",
    "| | | | | |\n",
    "| **Strengths** | • Medium-resolution stereo capture (2K)<br>• Wide field of view<br>• Robust SDK<br>• Good for objects without detailed text | • Lightweight and portable<br>• User-friendly design<br>• Built-in display for instant review<br>• 4K image capture capability<br>• Ideal for handheld workflows | • Compact industrial-grade system<br>• Accurate calibration<br>• Widely used in Isaac ROS<br>• Developed by NVIDIA camera team<br>• Professional-grade reliability | • Ultra-compact design<br>• Built-in 6DoF IMU<br>• HD depth sensing with Ultra mode<br>• Visual-inertial technology<br>• Aluminum frame for robustness<br>• USB Type-C connectivity |\n",
    "| **Weaknesses** | • Capture resolution not very high<br>• Cannot capture detailed texture<br>• Motion blur in video capturing<br>• Requires desktop for capturing | • No SDK for developers<br>• More consumer-focused<br>• Limited customization options | • Capture resolution not very high<br>• May not capture detailed texture<br>• Requires additional setup/integration<br>• Needs Jetson as additional device | • Smaller baseline (6.5cm)<br>• HD resolution (lower than 2K/4K)<br>• Requires powerful GPU for AR applications<br>• Limited depth range vs larger cameras |\n",
    "| | | | | |\n",
    "| **Setup Requirements** | | | | |\n",
    "| **Additional Hardware** | Desktop/laptop | None (standalone) | Jetson device | Desktop/laptop |\n",
    "| **Software Requirements** | ZED SDK | Companion mobile app | Custom integration, Isaac ROS | ZED SDK |\n",
    "| **Minimum System (SDK)** | Standard desktop | N/A | Jetson platform | Dual-core 2.3GHz, 4GB RAM, USB 3.0 |\n",
    "| **Recommended For** | • Desktop Development<br>• Robotics Integration<br>• SDK-based prototyping | • Field Data Collection<br>• Portable workflows<br>• Quick capture sessions | • Industrial Applications<br>• Isaac ROS projects<br>• Professional deployments | • Mixed-Reality Applications<br>• Compact Robotics<br>• Motion Tracking<br>• Space-constrained setups |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Time Reference\n",
    "\n",
    "Below are estimated running times for each stage of the 3D object reconstruction pipeline when using an NVIDIA RTX A6000 GPU with a dataset of 36 stereo frames at 4K (3000x4000) resolution:\n",
    "\n",
    "| Pipeline Stage | Estimated Time | Key Factors Affecting Performance |\n",
    "|----------------|----------------|----------------------------------|\n",
    "| **Initial Setup** | 1-2 minutes | Package installation and extension compilation |\n",
    "| **FoundationStereo Depth Estimation** | 1-2 minutes | Frame count, resolution |\n",
    "| **SAM2 Object Segmentation** | 25 Seconds | Frame count, resolution |\n",
    "| **Object Pose Tracking** | 3-4 minutes | Frame count, resolution |\n",
    "| **SDF Training** | 3-4 minutes | Training iterations, resolution, number of keyframes |\n",
    "| **Texture Baking** | 22-23 minutes | Texture resolution, mesh complexity, image resolution |\n",
    "| **Total Pipeline** | **31-32 minutes** | End-to-end processing time |\n",
    "\n",
    "**Notes:**\n",
    "- Performance scales approximately linearly with frame count\n",
    "- Higher resolution inputs increase processing time, particularly for Neural SDF training and texture generation\n",
    "- Complex objects with intricate geometry or challenging textures may require longer processing times\n",
    "- Using lower downscaling factors for higher quality outputs will increase processing time\n",
    "\n",
    "These estimates are based on benchmarks using the RTX A6000 GPU. Performance may vary based on system configuration, input data characteristics, and specific parameter settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Configuration and Data Setup\n",
    "\n",
    "Now let's set up our experiment configuration and load the sample dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import uuid\n",
    "import yaml \n",
    "import ipywidgets\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import JSON, display\n",
    "from PIL import Image \n",
    "import trimesh \n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Import custom modules for 3D reconstruction pipeline\n",
    "from nvidia.objectreconstruction.utils.visualization import (\n",
    "    create_stereo_viewer, create_depth_viewer, create_mask_viewer, \n",
    "    create_bbox_widget, create_3d_viewer\n",
    ")\n",
    "from nvidia.objectreconstruction.networks.foundationstereo import run_depth_estimation\n",
    "from nvidia.objectreconstruction.networks.sam2infer import run_mask_extraction\n",
    "from nvidia.objectreconstruction.dataloader import ReconstructionDataLoader\n",
    "from nvidia.objectreconstruction.networks import NVBundleSDF\n",
    "from nvidia.objectreconstruction.networks import ModelRendererOffscreen,vis_camera_poses\n",
    "\n",
    "def pretty_print_config(obj, title=None):\n",
    "    \"\"\"Pretty print configuration with custom object handling\"\"\"\n",
    "    \n",
    "    def json_serializer(obj):\n",
    "        \"\"\"Custom JSON serializer for non-serializable objects\"\"\"\n",
    "        if isinstance(obj, Path):\n",
    "            return str(obj)\n",
    "        elif hasattr(obj, '__dict__'):\n",
    "            return obj.__dict__\n",
    "        elif hasattr(obj, '_asdict'):  # namedtuples\n",
    "            return obj._asdict()\n",
    "        else:\n",
    "            return str(obj)\n",
    "    \n",
    "    if title:\n",
    "        print(f\"{title}\")\n",
    "        print(\"=\" * (len(title) + 4))\n",
    "    \n",
    "    # Convert to JSON-serializable format\n",
    "    json_str = json.dumps(obj, default=json_serializer, indent=2, sort_keys=True)\n",
    "    json_obj = json.loads(json_str)\n",
    "    \n",
    "    # Display using IPython's JSON widget with enhanced styling\n",
    "    return JSON(json_obj, expanded=True)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the experiment configuration file\n",
    "config_file_path = '/workspace/3d-object-reconstruction/data/configs/base.yaml'\n",
    "\n",
    "with open(config_file_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load the input dataset \n",
    "input_data_path = '/workspace/3d-object-reconstruction/data/samples/retail_item/'\n",
    "\n",
    "# Setup the experiment directory\n",
    "output_data_path = Path('/workspace/3d-object-reconstruction/data/output/retail_item/')\n",
    "\n",
    "# Check if output directory exists and ask user for action\n",
    "if output_data_path.exists():\n",
    "    clear_existing = input(f\"Output directory '{output_data_path}' already exists.\\nClear existing contents? (y/n): \").lower().strip()\n",
    "    \n",
    "    if clear_existing in ['y', 'yes']:\n",
    "        print(\"🗑️  Clearing existing output directory...\")\n",
    "        shutil.rmtree(output_data_path)\n",
    "        print(\"✅ Existing contents cleared!\")\n",
    "    elif clear_existing in ['n', 'no']:\n",
    "        print(\"📁 Keeping existing contents...\")\n",
    "    else:\n",
    "        print(\"⚠️  Invalid input. Keeping existing contents by default...\")\n",
    "\n",
    "# Create output directory and copy input frames\n",
    "output_data_path.mkdir(parents=True, exist_ok=True)\n",
    "shutil.copytree(input_data_path, output_data_path, dirs_exist_ok=True)\n",
    "\n",
    "# Update configuration to point to experiment directory\n",
    "config['workdir'] = str(output_data_path)\n",
    "config['bundletrack']['debug_dir'] = str(output_data_path)\n",
    "config['nerf']['save_dir'] = str(output_data_path)\n",
    "\n",
    "# Configure camera intrinsics and baseline for the sample dataset\n",
    "# The example dataset uses QooCam with the following specifications:\n",
    "# Intrinsic matrix format: [fx, 0, cx, 0, fy, cy, 0, 0, 1]\n",
    "# Baseline: distance between stereo camera lenses in meters\n",
    "config['camera_config']['intrinsic'] = [3079.6, 0, 2000.0, 0, 3075.1, 1500.01, 0, 0, 1]\n",
    "# The example dataset uses a step size of 1, which means we process all frames.\n",
    "config['camera_config']['step'] = 1 \n",
    "config['foundation_stereo']['intrinsic'] = config['camera_config']['intrinsic']\n",
    "config['foundation_stereo']['baseline'] = 0.0657696127  # 65.77mm baseline\n",
    "\n",
    "print(f\"Configuration loaded successfully!\")\n",
    "print(f\"Input data path: {input_data_path}\")\n",
    "print(f\"Output data path: {output_data_path}\")\n",
    "print(f\"Camera intrinsics configured for QooCam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Input Data\n",
    "\n",
    "Let's examine the sample stereo dataset to understand the input format and quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive stereo viewer to examine the input data\n",
    "stereo_viewer = create_stereo_viewer(output_data_path)\n",
    "display(stereo_viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Depth Estimation using FoundationStereo\n",
    "\n",
    "Now we'll extract depth information from our stereo image pairs using FoundationStereo. This step is crucial as it provides the 3D geometric foundation for our reconstruction pipeline.\n",
    "\n",
    "### About FoundationStereo\n",
    "\n",
    "FoundationStereo is a state-of-the-art neural network architecture designed for stereo depth estimation. It leverages:\n",
    "- **Transformer-based feature extraction** for robust matching across stereo pairs\n",
    "- **Multi-scale processing** to handle objects at different distances\n",
    "- **Uncertainty estimation** to identify reliable depth predictions\n",
    "\n",
    "The network takes left and right stereo images as input and produces dense depth maps with sub-pixel accuracy.\n",
    "\n",
    "### Configuration Review\n",
    "\n",
    "Let's examine the FoundationStereo configuration before running inference: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundationstereo_config = config['foundation_stereo']\n",
    "display(pretty_print_config(foundationstereo_config, \"FoundationStereo Configuration\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Depth Estimation\n",
    "\n",
    "Now let's run FoundationStereo inference on our stereo pairs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting FoundationStereo depth estimation...\")\n",
    "print(\"This may take several minutes depending on the number of frames and GPU performance.\")\n",
    "\n",
    "response = run_depth_estimation(\n",
    "    config=foundationstereo_config, \n",
    "    exp_path=output_data_path, \n",
    "    rgb_path=output_data_path / 'left',\n",
    "    depth_path=output_data_path / 'depth'\n",
    ")\n",
    "\n",
    "if response:\n",
    "    print(\"✓ FoundationStereo depth estimation completed successfully!\")\n",
    "else:\n",
    "    print(\"✗ Errors encountered during FoundationStereo inference.\")\n",
    "    print(\"Please check the configuration and input data before proceeding.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Depth Results\n",
    "\n",
    "Let's examine the generated depth maps to verify the quality of the depth estimation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive depth viewer\n",
    "depth_viewer = create_depth_viewer(output_data_path)\n",
    "display(depth_viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Object Segmentation using SAM2\n",
    "\n",
    "Next, we'll segment our target object across all frames using SAM2 (Segment Anything Model 2). This step is essential for isolating the object of interest from the background.\n",
    "\n",
    "### About SAM2\n",
    "\n",
    "SAM2 is Meta's advanced segmentation model that excels at:\n",
    "- **Video object tracking**: Maintaining consistent segmentation across frames\n",
    "- **Prompt-based segmentation**: Using minimal user input (like a bounding box) to identify objects\n",
    "- **Temporal consistency**: Leveraging motion and appearance cues for robust tracking\n",
    "\n",
    "The model requires only a single frame annotation (bounding box) and automatically propagates the segmentation to all other frames.\n",
    "\n",
    "### Interactive Bounding Box Selection\n",
    "\n",
    "Use the interactive widget below to draw a bounding box around your target object in the first frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "print(\"Instructions:\")\n",
    "print(\"1. Use your mouse to draw a bounding box around the target object\")\n",
    "print(\"2. Make sure the box tightly encompasses the entire object\")\n",
    "print(\"3. Click 'Finalize & Close' when satisfied with the bounding box\")\n",
    "\n",
    "bbox_widget = create_bbox_widget(output_data_path)\n",
    "display(bbox_widget.display())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SAM2 Segmentation\n",
    "\n",
    "Now we'll use the selected bounding box to run SAM2 segmentation across all frames:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bounding box coordinates\n",
    "x, y, w, h = bbox_widget.get_bbox()\n",
    "print(f\"Selected bounding box: x={x}, y={y}, width={w}, height={h}\")\n",
    "\n",
    "# Update SAM2 configuration with the bounding box coordinates\n",
    "sam2_config = config['sam2']\n",
    "sam2_config['bbox'] = [x, y, x+w, y+h] \n",
    "\n",
    "print(\"Starting SAM2 object segmentation...\")\n",
    "print(\"This process will track the object across all frames.\")\n",
    "\n",
    "# Run object segmentation using SAM2\n",
    "response = run_mask_extraction(\n",
    "    config=sam2_config,\n",
    "    exp_path=output_data_path,\n",
    "    rgb_path=output_data_path / \"left\",\n",
    "    mask_path=output_data_path / \"masks\"\n",
    ")\n",
    "\n",
    "if response:\n",
    "    print(\"✓ SAM2 segmentation completed successfully!\")\n",
    "else:\n",
    "    print(\"✗ Errors encountered during SAM2 inference.\")\n",
    "    print(\"Please check the bounding box selection and try again.\")\n",
    "\n",
    "assert response, 'SAM2 inference failed. Please resolve issues before proceeding.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the extracted masks from SAM2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_viewer = create_mask_viewer(output_data_path)\n",
    "display(mask_viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: 3D Reconstruction and Neural Rendering using NVBundleSDF\n",
    "\n",
    "This final step combines multiple state-of-the-art techniques to create a complete 3D reconstruction of your object. The pipeline integrates feature matching, pose estimation, and neural rendering to generate high-quality textured 3D assets.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "The 3D reconstruction process follows a sophisticated multi-stage approach:\n",
    "\n",
    "1. **Pose Estimation** → Estimate and optimize camera poses\n",
    "2. **Neural Reconstruction with Neural SDF** → Train a neural object field for 3D geometry\n",
    "3. **Texture Baking** → Generate production-ready textured meshes\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"../data/docs/bundlesdf_pipeline.png\" alt=\"BundleSDF Pipeline\" title=\"3D Reconstruction Pipeline\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "## Technical Components\n",
    "\n",
    "###BundleSDF\n",
    "[**BundleSDF: Neural 6-DOF Tracking and 3D Reconstruction**](https://arxiv.org/abs/2303.14158) combines:\n",
    "- **Volume Rendering**: Learns 3D geometry through differentiable ray casting\n",
    "- **Appearance Modeling**: Captures view-dependent effects and material properties\n",
    "- **SDF Representation**: Uses signed distance functions for clean mesh extraction\n",
    "- **Bundle Adjustment**: Performs global optimization across all frames for geometric consistency\n",
    "\n",
    "###FoundationStereo\n",
    "[**FoundationStereo: Zero-Shot Stereo Matching**](https://arxiv.org/abs/2501.09898) delivers robust depth estimation:\n",
    "- **Vision Foundation Model**: Leverages pre-trained vision transformers for rich feature extraction\n",
    "- **Zero-Shot Generalization**: Performs well across diverse environments without domain-specific fine-tuning\n",
    "- **Multi-Scale Processing**: Handles objects at different distances through hierarchical feature analysis\n",
    "- **Sub-Pixel Accuracy**: Achieves precise depth measurements with transformer-based stereo matching\n",
    "\n",
    "###RoMa Feature Matching\n",
    "[**RoMa: A Robust Dense Feature Matching**](https://arxiv.org/abs/2305.15404) provides reliable feature correspondences between frames:\n",
    "- **Dense Matching**: Establishes pixel-to-pixel correspondences across viewpoints\n",
    "- **Robust Descriptors**: Uses transformer-based features for challenging lighting and viewpoint changes\n",
    "- **Uncertainty Estimation**: Provides confidence scores for each match to filter unreliable correspondences\n",
    "\n",
    "###SAM2\n",
    "[**SAM2: Segment Anything in Images and Videos**](https://arxiv.org/abs/2408.00714) extends segmentation to video:\n",
    "- **Transformer Architecture**: Uses hierarchical vision transformer with streaming memory\n",
    "- **Temporal Consistency**: Maintains object tracking across frames via memory mechanisms\n",
    "- **Prompt Flexibility**: Accepts points, boxes, and masks for interactive segmentation\n",
    "- **Real-time Performance**: Processes video 6× faster than the original SAM\n",
    "\n",
    "## Configuration Review\n",
    "\n",
    "Let's examine the configurations for each component before running the reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roma_config = config['roma']\n",
    "display(pretty_print_config(roma_config, \"ROMA Feature Matching Configuration\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['bundletrack']['debug_dir'] = output_data_path / \"bundletrack\"\n",
    "bundletrack_config = config['bundletrack']\n",
    "display(pretty_print_config(bundletrack_config, \"Pose Estimation Configuration\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['nerf']['save_dir'] = output_data_path #sdf config\n",
    "nerf_config = config['nerf']\n",
    "display(pretty_print_config(nerf_config, \"SDF Training Configuration\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texture Baking\n",
    "texturebake_config = config['texture_bake']\n",
    "display(pretty_print_config(texturebake_config, \"Texture Baking Configuration\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataloaders\n",
    "track_dataset = ReconstructionDataLoader(\n",
    "    str(output_data_path), \n",
    "    config, \n",
    "    downscale=bundletrack_config['downscale'],\n",
    "    min_resolution=bundletrack_config['min_resolution']\n",
    ")\n",
    "nerf_dataset = ReconstructionDataLoader(\n",
    "    str(output_data_path), \n",
    "    config, \n",
    "    downscale=nerf_config['downscale'],\n",
    "    min_resolution=nerf_config['min_resolution']\n",
    ")\n",
    "texture_dataset = ReconstructionDataLoader(\n",
    "    str(output_data_path), \n",
    "    config, \n",
    "    downscale=texturebake_config['downscale'],\n",
    "    min_resolution=texturebake_config['min_resolution']\n",
    ")\n",
    "\n",
    "# Setup NVBundleSDF instance \n",
    "tracker = NVBundleSDF(nerf_config, bundletrack_config, roma_config,texturebake_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now continue with feature matching and pose estimation using BundleSDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run bundle track for feature matching and pose estimation \n",
    "tracker.run_track(track_dataset)\n",
    "\n",
    "if not os.path.exists(os.path.join(config['bundletrack']['debug_dir'], 'keyframes.yml')):\n",
    "    print(f'Feature Matching and Pose Estimation Failed, please check logs and resolve error before proceeding.')\n",
    "else:\n",
    "    print(f'Feature Matching and Pose Estimation successful.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the camera poses estimation results. \n",
    "+ The `scale` parameter controls the size of the camera frustums in the visualization - smaller values like 0.01 make the cameras appear smaller. \n",
    "+ The `eps` parameter affects point cloud downsampling during visualization - smaller values like 0.01 preserve more detail but may be slower to render.\n",
    "+ The visualization shows camera positions and orientations as colored coordinate axes (red=X-right, green=Y-up, blue=Z-lookat), with yellow lines indicating the camera viewing frustums.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = vis_camera_poses(os.path.join(config['bundletrack']['debug_dir'], 'keyframes.yml'),track_dataset,scale=0.03,eps=0.01)\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted pose information and keyframes, we can train our SDF model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SDF training \n",
    "tracker.run_global_sdf(nerf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our SDF model trained, let us proceed on texture baking and here we can customize our scale factor to use the original scale of the images if needed for 4k images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.run_texture_bake(texture_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our 3d textured asset, let us take a look at the generated asset to see how it looks ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(f'{output_data_path}/material_0.png')\n",
    "mesh = trimesh.load(f'{output_data_path}/textured_mesh.obj',process=False)\n",
    "tex = trimesh.visual.TextureVisuals(image=im)\n",
    "mesh.visual.texture = tex\n",
    "view_mesh = mesh\n",
    "material = mesh.visual.material\n",
    "material.diffuse = [255,255,255,255]\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us also observe the mesh in its reconstructed lightning below.\n",
    "K,H,W = nerf_dataset.K, nerf_dataset.H, nerf_dataset.W\n",
    "tRes = 800\n",
    "scale = tRes/max(H,W)\n",
    "H,W = int(H*scale), int(W*scale)\n",
    "cam_K = K[:2]*scale\n",
    "try:\n",
    "    renderer = ModelRendererOffscreen([],cam_K,H,W)\n",
    "    renderer.add_mesh(mesh)\n",
    "    colors,depths = renderer.render_fixed_cameras()\n",
    "except:\n",
    "    renderer = ModelRendererOffscreen([],cam_K,H,W)\n",
    "    renderer.add_mesh(mesh)\n",
    "    colors,depths = renderer.render_fixed_cameras()\n",
    "\n",
    "plt.figure()\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(colors[i])\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### **Workflow Summary**\n",
    "\n",
    "Congratulations! You have successfully completed the end-to-end 3D object reconstruction pipeline. Here's what we accomplished:\n",
    "\n",
    "#### **Pipeline Achievements**\n",
    "1. **✅ Depth Estimation**: Generated accurate depth maps from stereo pairs using FoundationStereo's transformer-based architecture\n",
    "2. **✅ Object Segmentation**: Created consistent object masks across all frames using SAM2's video tracking capabilities\n",
    "3. **✅ Pose Estimation**: Estimated and optimized camera poses for the next step reconstruction \n",
    "4. **✅ Neural Reconstruction**: Trained a Neural Field to capture the object's 3D geometry\n",
    "5. **✅ Texture Baking**: Generated high-resolution texture maps and exported production-ready 3D assets\n",
    "\n",
    "#### **Generated Assets**\n",
    "Your reconstruction pipeline has produced the following outputs in the experiment directory:\n",
    "- **`textured_mesh.obj`**: Complete 3D mesh with UV mapping\n",
    "- **`material_0.png`**: High-resolution texture map\n",
    "- **`keyframes.yaml`**: Optimized camera poses for each frame\n",
    "- **`depth/`**: Dense depth maps for all input frames\n",
    "- **`masks/`**: Object segmentation masks for background removal\n",
    "\n",
    "#### **Export to USD**\n",
    "- In order to support direct loading of various file types into Omniverse, we provide a set of converters that can convert the file into a USD file.\n",
    "- [USD Converter using isaaclab.sim.converters](https://isaac-sim.github.io/IsaacLab/main/source/api/lab/isaaclab.sim.converters.html)\n",
    "\n",
    "### **Integration with other workflows**\n",
    "\n",
    "The generated 3D assets are immediately ready for integration into various platforms and workflows:\n",
    "\n",
    "**Applications:**\n",
    "- **Robotic Manipulation**: Use reconstructed objects for grasping and manipulation training\n",
    "- **Sim2Real Transfer**: Bridge the gap between simulation and real-world deployment\n",
    "- **Digital Twins**: Create accurate digital replicas of real-world objects\n",
    "- **Computer Vision Training**: Generate labeled datasets with your reconstructed objects\n",
    "- **Domain Adaptation**: Create variations of real objects for robust model training\n",
    "- **Rare Object Simulation**: Generate synthetic data for objects that are difficult to collect\n",
    "\n",
    "**Further Reading**\n",
    "- [Object Detection Synthetic Data Generation using isaacsim.replicator.object](https://docs.isaacsim.omniverse.nvidia.com/4.5.0/replicator_tutorials/tutorial_replicator_object.html)\n",
    "  \n",
    "### **Advanced Customization Options**\n",
    "\n",
    "#### **Quality Optimization**\n",
    "- **Higher Resolution**: Modify `texture_bake.downscale` to `1` for full-resolution texture baking\n",
    "- **Extended Training**: Increase NeRF training iterations for improved reconstruction quality\n",
    "- **Custom Camera Intrinsics**: Adapt the pipeline for different camera setups\n",
    "\n",
    "#### **Experiment with Your Own Data**\n",
    "1. **Capture Guidelines**: Follow the data collection best practices demonstrated in Step 1\n",
    "2. **Camera Calibration**: Ensure accurate intrinsic parameters for your stereo setup\n",
    "3. **Lighting Conditions**: Experiment with different lighting setups for optimal results\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "You now have a complete understanding of the 3D object reconstruction pipeline and practical experience with state-of-the-art computer vision techniques. The generated assets are production-ready and can be immediately integrated into your robotics, gaming, or AI workflows.\n",
    "\n",
    "The combination of FoundationStereo, SAM2, and BundleSDF provides a robust foundation for creating high-quality 3D content from real-world objects, bridging the gap between physical and digital worlds.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
